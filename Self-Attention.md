**Self-Attention** este un mecanism care permite unui model să acorde **importanță diferită** fiecărui element dintr-o secvență atunci când procesează un anumit element.  
Este fundamental pentru arhitectura **[[Transformer]]**, permițând modelului să capteze relații între toate pozițiile dintr-o secvență, indiferent de distanță.

## Intuiție

Pentru fiecare cuvânt dintr-o propoziție, modelul decide **câtă atenție** să acorde tuturor celorlalte cuvinte atunci când îl procesează.
