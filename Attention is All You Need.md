Lucrare publicată în 2017 de Vaswani et al., propunând o nouă arhitectură pentru procesarea secvențelor – **[[Transformer]]Transformer** – care **elimină complet recurența** și se bazează exclusiv pe mecanismul de **atenție**.

## Inovații cheie

- Introduce **[[Transformer]]**, o arhitectură complet bazată pe **[[Self-Attention]]**
- Înlocuiește rețelele recurente ( RNN, LSTM) cu operații paralele eficiente
- Permite **antrenarea rapidă** pe seturi mari de date
- Scalabil și performant pentru sarcini NLP complexe
- 